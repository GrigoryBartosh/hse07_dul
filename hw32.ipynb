{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3.2 High-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH_DATA = os.path.join('data', 'hw3-q2.pkl')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATA, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    \n",
    "xs_train = dataset['train']\n",
    "xs_val = dataset['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def collate_fn(xs):\n",
    "    xs = torch.tensor(xs, dtype=torch.float32)\n",
    "    xs = xs * 2 / 255 - 1\n",
    "    xs = xs.permute(0, 3, 1, 2)\n",
    "    return xs\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=xs_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=xs_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedShortcutConnection(nn.Module):\n",
    "    def __init__(self, in_channels=128):\n",
    "        super(GatedShortcutConnection, self).__init__()\n",
    "                 \n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "                 \n",
    "    def forward(self, x):\n",
    "        return self.conv1(x) * self.sigmoid(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels=64):\n",
    "        super(ResidualStack, self).__init__()\n",
    "                 \n",
    "        layers = []\n",
    "        for _ in range(5):\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels, out_channels=64, kernel_size=3, padding=1, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64, out_channels=64 * 2, kernel_size=3, padding=1, bias=False),\n",
    "                nn.ReLU(),\n",
    "                GatedShortcutConnection(in_channels=64 * 2),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "            in_channels = 64 * 2\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "                 \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            ResidualStack(in_channels=128),\n",
    "            nn.Conv2d(128, 256, kernel_size=1)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "                 \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.reshape(-1, 2, 128 * 8 * 8)\n",
    "        mu, log_var = x[:, 0, :], x[:, 1, :]\n",
    "        var = torch.exp(log_var)\n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            ResidualStack(in_channels=128),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "                 \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 128, 8, 8)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def reparameterize(self, mu, var):\n",
    "        eps = torch.randn_like(mu, device=device)\n",
    "        z = mu + eps * torch.sqrt(var)\n",
    "        return z\n",
    "            \n",
    "    def encode(self, x):\n",
    "        mu, var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, var)\n",
    "        return mu, var, z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu, var, z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return mu, var, x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        loss = ((x - target) ** 2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, mu, var):\n",
    "        kl_div = 1.0 + torch.log(var) - mu ** 2 - var\n",
    "        kl_div = -0.5 * kl_div\n",
    "        loss = kl_div.sum(dim=1)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion_mse, criterion_kl,\n",
    "          optimizer, scheduler, epochs):\n",
    "    losses_mse_train = []\n",
    "    losses_kl_train = []\n",
    "    losses_train = []\n",
    "    losses_mse_val = []\n",
    "    losses_kl_val = []\n",
    "    losses_val = []\n",
    "    for i in trange(epochs):\n",
    "        losses_mse = []\n",
    "        losses_kl = []\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for xs in train_data_loader:\n",
    "            xs = xs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mu, var, out = model(xs)\n",
    "            \n",
    "            loss_mse = criterion_mse(out, xs)\n",
    "            loss_kl = criterion_kl(mu, var)\n",
    "            loss = loss_mse + loss_kl\n",
    "\n",
    "            losses_mse.append(loss_mse.item())\n",
    "            losses_kl.append(loss_kl.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_mse_train.append(np.array(losses_mse).mean())\n",
    "        losses_kl_train.append(np.array(losses_kl).mean())\n",
    "        losses_train.append(np.array(losses).mean())\n",
    "\n",
    "        losses_mse = []\n",
    "        losses_kl = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xs in val_data_loader:\n",
    "                xs = xs.to(device)\n",
    "\n",
    "                mu, var, out = model(xs)\n",
    "\n",
    "                loss_mse = criterion_mse(out, xs)\n",
    "                loss_kl = criterion_kl(mu, var)\n",
    "                loss = loss_mse + loss_kl\n",
    "\n",
    "                losses_mse.append(loss_mse.item())\n",
    "                losses_kl.append(loss_kl.item())\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        losses_mse_val.append(np.array(losses_mse).mean())\n",
    "        losses_kl_val.append(np.array(losses_kl).mean())\n",
    "        losses_val.append(np.array(losses).mean())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    plt.plot(range(epochs), losses_mse_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_mse_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss mse')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), losses_kl_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_kl_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss KL')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_interpolation(model, n=5, m=10):\n",
    "    sample_ids = list(range(n))\n",
    "    samples_a = xs_train[sample_ids]\n",
    "    samples_b = xs_train[sample_ids[-1:] + sample_ids[:-1]]\n",
    "    samples_a = collate_fn(samples_a).to(device)\n",
    "    samples_b = collate_fn(samples_b).to(device)\n",
    "    \n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            a, b = samples_a[i], samples_b[i]\n",
    "            (za, _, _), (zb, _, _) = model.encode(a[None, :]), model.encode(b[None, :])\n",
    "            for j in range(m):\n",
    "                x = model.decode(za + (zb - za) * j / (m - 1))\n",
    "                x = x.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "                x = x[:, :, [2, 1, 0]] # TODO\n",
    "                images.append(x)\n",
    "            \n",
    "    images = np.array(images)\n",
    "    images = (images + 1) / 2\n",
    "    images = np.clip(images, 0, 1)\n",
    "    \n",
    "    f, axarr = plt.subplots(n, m)\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(15)\n",
    "    for i, img in enumerate(images):\n",
    "        axarr[n - 1 - i // m, i % m].imshow(img)\n",
    "        axarr[n - 1 - i // m, i % m].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sampels(model, n=100):\n",
    "    z = torch.randn([n, 128 * 8 * 8]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = model.decode(z)\n",
    "        images = images.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        images = images[:, :, :, [2, 1, 0]] # TODO\n",
    "        \n",
    "    images = (images + 1) / 2\n",
    "    images = np.clip(images, 0, 1)\n",
    "    \n",
    "    n = int(np.ceil(np.sqrt(n)))\n",
    "    f, axarr = plt.subplots(n, n)\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(15)\n",
    "    for i, img in enumerate(images):\n",
    "        axarr[i // n, i % n].imshow(img)\n",
    "        axarr[i // n, i % n].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 24\n",
    "lr = 0.0002\n",
    "w_l2 = 0\n",
    "\n",
    "model = VAE()\n",
    "model.to(device)\n",
    "\n",
    "criterion_mse = MSELoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_mse, criterion_kl, optimizer, None, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "show_interpolation(model, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sampels(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
