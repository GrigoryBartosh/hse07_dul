{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH_DATA = os.path.join('data', 'hw2_q2.pkl')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATA, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    \n",
    "xs_train = dataset['train']\n",
    "xs_val = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def collate_fn(xs):\n",
    "    xs = torch.LongTensor(xs)\n",
    "    xs = xs.permute(0, 3, 1, 2)\n",
    "    return xs\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=xs_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data_loader = data.DataLoader(\n",
    "    dataset=xs_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3,\n",
    "        stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv3x3(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, in_planes)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activ(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.activ(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_planes, planes, layers_cnt):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        layers = [conv3x3(in_planes, planes)]\n",
    "        layers += [ResBlock(planes, planes) for _ in range(layers_cnt)]\n",
    "        layers += [conv3x3(planes, 2 * in_planes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, mask, in_planes, h, w, planes, layers):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "        \n",
    "        self.mask = mask.clone()\n",
    "        \n",
    "        self.resnet = ResNet(in_planes, planes, layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.mask * x\n",
    "        log_s, t = self.resnet(z).split(z.shape[1], dim=1)\n",
    "        log_s = torch.tanh(log_s)\n",
    "        z = z + (1 - self.mask) * (torch.exp(log_s) * x + t)\n",
    "        logdet = (log_s * (1.0 - self.mask)).flatten(1).sum(dim=1, keepdim=True)\n",
    "        return z, logdet\n",
    "            \n",
    "    def reconstruction(self, x):\n",
    "        z = self.mask * x\n",
    "        log_s, t = self.resnet(z).split(z.shape[1], dim=1)\n",
    "        log_s = torch.tanh(log_s)\n",
    "        z = z + (1 - self.mask) * (x - t) / torch.exp(log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    EPS = 1e-9\n",
    "    \n",
    "    def __init__(self, c, h, w):\n",
    "        super(ActNorm, self).__init__()\n",
    "        \n",
    "        self.w = nn.Parameter(torch.ones([1, c, h, w]))\n",
    "        self.b = nn.Parameter(torch.zeros([1, c, h, w]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * self.w + self.b\n",
    "        logdet = torch.log(self.w.abs() + ActNorm.EPS).flatten(1).sum(dim=1, keepdim=True)\n",
    "        return x, logdet\n",
    "    \n",
    "    def reconstruction(self, x):\n",
    "        x = (x - self.b) / self.w\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    def __init__(self, in_planes, h, w, planes=32, layers=4):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.layers1 = nn.ModuleList()\n",
    "        mask = self.get_checkerboard_mask(in_planes, h, w)\n",
    "        for _ in range(4):\n",
    "            self.layers1.append(CouplingLayer(mask, in_planes, h, w, planes, layers))\n",
    "            self.layers1.append(ActNorm(in_planes, h, w))\n",
    "            mask = 1.0 - mask\n",
    "            \n",
    "        in_planes, h, w = in_planes * 4, h // 2, w // 2\n",
    "        \n",
    "        self.layers2 = nn.ModuleList()\n",
    "        mask = self.get_channel_split_mask(in_planes, h, w)\n",
    "        for _ in range(3):\n",
    "            self.layers2.append(CouplingLayer(mask, in_planes, h, w, planes, layers))\n",
    "            self.layers2.append(ActNorm(in_planes, h, w))\n",
    "            mask = 1.0 - mask\n",
    "        \n",
    "        self.layers3 = nn.ModuleList()\n",
    "        mask = self.get_checkerboard_mask(in_planes, h, w)\n",
    "        for _ in range(3):\n",
    "            self.layers3.append(CouplingLayer(mask, in_planes, h, w, planes, layers))\n",
    "            self.layers3.append(ActNorm(in_planes, h, w))\n",
    "            mask = 1.0 - mask\n",
    "            \n",
    "        in_planes, h, w = in_planes * 4, h // 2, w // 2\n",
    "            \n",
    "        self.layers4 = nn.ModuleList()\n",
    "        mask = self.get_channel_split_mask(in_planes, h, w)\n",
    "        for _ in range(3):\n",
    "            self.layers4.append(CouplingLayer(mask, in_planes, h, w, planes, layers))\n",
    "            self.layers4.append(ActNorm(in_planes, h, w))\n",
    "            mask = 1.0 - mask\n",
    "            \n",
    "        self.layers5 = nn.ModuleList()\n",
    "        mask = self.get_checkerboard_mask(in_planes, h, w)\n",
    "        for i in range(3):\n",
    "            self.layers5.append(CouplingLayer(mask, in_planes, h, w, planes, layers))\n",
    "            self.layers5.append(ActNorm(in_planes, h, w))\n",
    "            mask = 1.0 - mask\n",
    "            \n",
    "    def get_checkerboard_mask(self, c, h, w):\n",
    "        a = np.ones([1, c, h, w], dtype=np.bool)\n",
    "        b = np.ones([1, c, h, w], dtype=np.bool)\n",
    "        a[:, :, np.arange(1, h, 2), :] = False \n",
    "        b[:, :, :, np.arange(0, w, 2)] = False\n",
    "        return torch.tensor(a ^ b, dtype=torch.float32)\n",
    "\n",
    "    def get_channel_split_mask(self, c, h, w):\n",
    "        mask = torch.zeros([1, c, h, w], dtype=torch.float32)\n",
    "        idx = t = np.arange(0, c // 4) * 4\n",
    "        idx = np.stack((idx, idx + 1), axis=1).reshape(-1)\n",
    "        mask[:, idx, :, :] = 1.0\n",
    "        return mask\n",
    "            \n",
    "    def logit(self, x):\n",
    "        z = torch.log(x) - torch.log(1.0 - x)\n",
    "        diag = (1.0 / x + 1.0 / (1.0 - x)).abs().flatten(start_dim=1)\n",
    "        logdet = torch.log(diag + RealNVP.EPS).sum(dim=1, keepdim=True)\n",
    "        return z, logdet    \n",
    "        \n",
    "    def logit_reconstruction(self, x):\n",
    "        z = torch.exp(x)\n",
    "        return z / (z + 1.0)\n",
    "\n",
    "    def dequantize(self, x, alpha=0.05):\n",
    "        z = alpha + (1.0 - alpha) * x / 4.0\n",
    "        diag = (torch.ones_like(x) * (1.0 - alpha) / 4.0).flatten(start_dim=1)\n",
    "        logdet = torch.log(diag.abs()).sum(dim=1, keepdim=True)\n",
    "        return z, logdet\n",
    "        \n",
    "    def dequantize_reconstruction(self, x, alpha=0.05):\n",
    "        return 4.0 * (x - alpha) / (1.0 - alpha)\n",
    "\n",
    "    def preprocess(self, x, alpha=0.05):\n",
    "        deq, logdet1 = self.dequantize(x, alpha)\n",
    "        z, logdet2 = self.logit(deq)\n",
    "        logdet = logdet1 + logdet2\n",
    "        return z, logdet\n",
    "        \n",
    "    def preprocess_reconstruction(self, x, alpha=0.05):\n",
    "        z = self.logit_reconstruction(x)\n",
    "        z = self.dequantize_reconstruction(z, alpha)\n",
    "        return z\n",
    "    \n",
    "    def squeeze(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        return F.unfold(x, (2, 2), stride=2).reshape(n, 4 * c, h // 2, w // 2)\n",
    "\n",
    "    def unsqueeze(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        return F.fold(x.reshape(n, c, -1), (h * 2, w * 2), (2, 2), stride=2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x, logdet = self.preprocess(x)\n",
    "        \n",
    "        for layer in self.layers1:\n",
    "            x, x_logdet = layer(x)\n",
    "            logdet = logdet + x_logdet\n",
    "\n",
    "        x = self.squeeze(x)\n",
    "        \n",
    "        for layer in self.layers2:\n",
    "            x, x_logdet = layer(x)\n",
    "            logdet = logdet + x_logdet\n",
    "        for layer in self.layers3:\n",
    "            x, x_logdet = layer(x)\n",
    "            logdet = logdet + x_logdet\n",
    "\n",
    "        x = self.squeeze(x)\n",
    "        \n",
    "        for layer in self.layers4:\n",
    "            x, x_logdet = layer(x)\n",
    "            logdet = logdet + x_logdet\n",
    "        for layer in self.layers5:\n",
    "            x, x_logdet = layer(x)\n",
    "            logdet = logdet + x_logdet\n",
    "\n",
    "        x = self.unsqueeze(x)\n",
    "        x = self.unsqueeze(x)\n",
    "        \n",
    "        return x, torch.exp(logdet)\n",
    "        \n",
    "    def reconstruction(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = self.squeeze(x)\n",
    "        \n",
    "        for layer in reversed(self.layers5):\n",
    "            x = layer.reconstruction(x)\n",
    "        for layer in reversed(self.layers4):\n",
    "            x = layer.reconstruction(x)\n",
    "            \n",
    "        x = self.unsqueeze(x)\n",
    "        \n",
    "        for layer in reversed(self.layers3):\n",
    "            x = layer.reconstruction(x)\n",
    "        for layer in reversed(self.layers2):\n",
    "            x = layer.reconstruction(x)\n",
    "            \n",
    "        x = self.unsqueeze(x)\n",
    "        \n",
    "        for layer in reversed(self.layers1):\n",
    "            x = layer.reconstruction(x)\n",
    "            \n",
    "        x = self.preprocess_reconstruction(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLLELoss(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(MLLELoss, self).__init__()\n",
    "\n",
    "    def forward(self, z, dens):\n",
    "        m = D.normal.Normal(0, 1)\n",
    "        log_pz = m.log_prob(z) \n",
    "        log_pz = log_pz.flatten(start_dim=1).sum(dim=1, keepdim=True)\n",
    "        loss = -log_pz - torch.log(dens)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epochs):\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for i in trange(epochs):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for xs in train_data_loader:\n",
    "            xs = xs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z, dens = model(xs)\n",
    "            loss = criterion(z, dens)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_train.append(np.array(losses).mean())\n",
    "\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xs in val_data_loader:\n",
    "                xs = xs.to(device)\n",
    "                \n",
    "                z, dens = model(xs)\n",
    "                loss = criterion(z, dens)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        losses_val.append(np.array(losses).mean())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "w_l2 = 0\n",
    "\n",
    "model = RealNVP(3, 32, 32, planes=32, layers=4)\n",
    "model.to(device)\n",
    "\n",
    "criterion = MLLELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion, optimizer, None, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
