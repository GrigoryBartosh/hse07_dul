{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3.1 VAEs in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_1():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + rand.randn(count, 2) * [[5.0, 1.0]]\n",
    "\n",
    "\n",
    "def sample_data_2():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + (rand.randn(count, 2) * [[5.0, 1.0]]).dot(\n",
    "    [[np.sqrt(2) / 2, np.sqrt(2) / 2], [-np.sqrt(2) / 2, np.sqrt(2) / 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = sample_data_1()\n",
    "\n",
    "plt.plot(dataset1[:, 0], dataset1[:, 1], 'o')\n",
    "plt.show()\n",
    "\n",
    "n = dataset1.shape[0]\n",
    "n_train = int(n * 0.8)\n",
    "xs1_train, xs1_val = dataset1[:n_train], dataset1[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = sample_data_2()\n",
    "\n",
    "plt.plot(dataset2[:, 0], dataset2[:, 1], 'o')\n",
    "plt.show\n",
    "\n",
    "n = dataset2.shape[0]\n",
    "n_train = int(n * 0.8)\n",
    "xs2_train, xs2_val = dataset2[:n_train], dataset2[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "def collate_fn(data):\n",
    "    xs = data\n",
    "    xs = torch.tensor(xs, dtype=torch.float32)\n",
    "    return xs\n",
    "\n",
    "train_data1_loader = data.DataLoader(\n",
    "    dataset=xs1_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data1_loader = data.DataLoader(\n",
    "    dataset=xs1_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_data2_loader = data.DataLoader(\n",
    "    dataset=xs2_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data2_loader = data.DataLoader(\n",
    "    dataset=xs2_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_dims, activ=nn.ReLU, last_active=False, dropout_rate=0):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim, layer_dims = layer_dims[0], layer_dims[1:]\n",
    "        for dim in layer_dims[:-1]:\n",
    "            layers += [\n",
    "                nn.Linear(in_dim, dim),\n",
    "                activ(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "            in_dim = dim\n",
    "\n",
    "        layers += [nn.Linear(in_dim, layer_dims[-1])]\n",
    "        if last_active:\n",
    "            layers += [activ()]\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.mlp_encoder = MLP(layer_dims)\n",
    "        self.mlp_decoder = MLP(layer_dims)\n",
    "        \n",
    "    def get_mu_var(self, mlp, x):\n",
    "        x = mlp(x)\n",
    "        mu, log_var = x[:, :2], x[:, 2:]\n",
    "        var = torch.exp(log_var)\n",
    "        return mu, var\n",
    "            \n",
    "    def encode(self, x):\n",
    "        return self.get_mu_var(self.mlp_encoder, x)\n",
    "    \n",
    "    def reparameterize(self, mu, var):\n",
    "        eps = torch.randn_like(mu).to(device)\n",
    "        z = mu + eps * torch.sqrt(var)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.get_mu_var(self.mlp_decoder, z)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu_z, var_z = self.encode(x)\n",
    "        z = self.reparameterize(mu_z, var_z)\n",
    "        mu_x, var_x = self.decode(z)\n",
    "        return mu_z, var_z, mu_x, var_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.mlp_encoder = MLP(layer_dims)\n",
    "        self.mlp_decoder = MLP(layer_dims)\n",
    "        \n",
    "    def get_mu_var(self, mlp, x):\n",
    "        x = mlp(x)\n",
    "        mu, log_var = x[:, :2], x[:, 2:]\n",
    "        log_var = log_var.expand(-1, 2)\n",
    "        var = torch.exp(log_var)\n",
    "        return mu, var\n",
    "            \n",
    "    def encode(self, x):\n",
    "        return self.get_mu_var(self.mlp_encoder, x)\n",
    "    \n",
    "    def reparameterize(self, mu, var):\n",
    "        eps = torch.randn_like(mu).to(device)\n",
    "        z = mu + eps * torch.sqrt(var)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.get_mu_var(self.mlp_decoder, z)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu_z, var_z = self.encode(x)\n",
    "        z = self.reparameterize(mu_z, var_z)\n",
    "        mu_x, var_x = self.decode(z)\n",
    "        return mu_z, var_z, mu_x, var_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, mu, var):\n",
    "        prob = -0.5 * (torch.log(np.pi * 2 * var) + torch.pow(x - mu, 2) / var)\n",
    "        loss = -prob.sum(dim=1)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, mu, var):\n",
    "        kl_div = 1.0 + torch.log(var) - mu ** 2 - var\n",
    "        kl_div = -0.5 * kl_div\n",
    "        loss = kl_div.sum(dim=1)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion_prob, criterion_kl,\n",
    "          optimizer, scheduler, epochs,\n",
    "          train_data_loader, val_data_loader):\n",
    "    losses_prob_train = []\n",
    "    losses_kl_train = []\n",
    "    losses_train = []\n",
    "    losses_prob_val = []\n",
    "    losses_kl_val = []\n",
    "    losses_val = []\n",
    "    for i in trange(epochs):\n",
    "        losses_prob = []\n",
    "        losses_kl = []\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for xs in train_data_loader:\n",
    "            if len(xs) == 2:\n",
    "                xs, _ = xs\n",
    "                \n",
    "            xs = xs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mu_z, var_z, mu_x, var_x = model(xs)\n",
    "            \n",
    "            loss_prob = criterion_prob(xs, mu_x, var_x)\n",
    "            loss_kl = criterion_kl(mu_z, var_z)\n",
    "            loss = loss_prob + loss_kl\n",
    "\n",
    "            losses_prob.append(loss_prob.item())\n",
    "            losses_kl.append(loss_kl.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_prob_train.append(np.array(losses_prob).mean())\n",
    "        losses_kl_train.append(np.array(losses_kl).mean())\n",
    "        losses_train.append(np.array(losses).mean())\n",
    "\n",
    "        losses_prob = []\n",
    "        losses_kl = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xs in val_data_loader:\n",
    "                if len(xs) == 2:\n",
    "                    xs, _ = xs\n",
    "\n",
    "                xs = xs.to(device)\n",
    "\n",
    "                mu_z, var_z, mu_x, var_x = model(xs)\n",
    "\n",
    "                loss_prob = criterion_prob(xs, mu_x, var_x)\n",
    "                loss_kl = criterion_kl(mu_z, var_z)\n",
    "                loss = loss_prob + loss_kl\n",
    "\n",
    "                losses_prob.append(loss_prob.item())\n",
    "                losses_kl.append(loss_kl.item())\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        losses_prob_val.append(np.array(losses_prob).mean())\n",
    "        losses_kl_val.append(np.array(losses_kl).mean())\n",
    "        losses_val.append(np.array(losses).mean())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    plt.plot(range(epochs), losses_prob_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_prob_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss prob')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), losses_kl_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_kl_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss KL')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), losses_train, label=\"train\")\n",
    "    plt.plot(range(epochs), losses_val, label=\"val\")\n",
    "    plt.xlabel('epoch num')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, n=100000):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn([n, 2], device=device)\n",
    "        mu_x, var_x = model.decode(z)\n",
    "        eps = torch.randn([n, 2], device=device)\n",
    "        x = eps * torch.sqrt(var_x) + mu_x\n",
    "        mu_x = mu_x.cpu().numpy()\n",
    "        x = x.cpu().numpy()\n",
    "        \n",
    "    plt.plot(x[:, 0], x[:, 1], 'o', label=\"X\")\n",
    "    plt.plot(mu_x[:, 0], mu_x[:, 1], 'o', label=\"Mean\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая модель, первый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "w_l2 = 0\n",
    "\n",
    "model = Net1([2, 64, 64, 4])\n",
    "model.to(device)\n",
    "\n",
    "criterion_prob = ProbLoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_prob, criterion_kl,\n",
    "      optimizer, None, epochs,\n",
    "      train_data1_loader, val_data1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая модель, второй датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "w_l2 = 0\n",
    "\n",
    "model = Net1([2, 64, 64, 4])\n",
    "model.to(device)\n",
    "\n",
    "criterion_prob = ProbLoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_prob, criterion_kl,\n",
    "      optimizer, None, epochs,\n",
    "      train_data2_loader, val_data2_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая модель, первый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "w_l2 = 0\n",
    "\n",
    "model = Net2([2, 64, 64, 3])\n",
    "model.to(device)\n",
    "\n",
    "criterion_prob = ProbLoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_prob, criterion_kl,\n",
    "      optimizer, None, epochs,\n",
    "      train_data1_loader, val_data1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая модель, второй датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "w_l2 = 0\n",
    "\n",
    "model = Net2([2, 64, 64, 3])\n",
    "model.to(device)\n",
    "\n",
    "criterion_prob = ProbLoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_prob, criterion_kl,\n",
    "      optimizer, None, epochs,\n",
    "      train_data2_loader, val_data2_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_3():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = [[-1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    b = [[1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    c = np.c_[2 * np.cos(np.linspace(0, np.pi, count // 3)),\n",
    "    -np.sin(np.linspace(0, np.pi, count // 3))]\n",
    "\n",
    "    c += rand.randn(*c.shape) * 0.2\n",
    "    data_x = np.concatenate([a, b, c], axis=0)\n",
    "    data_y = np.array([0] * len(a) + [1] * len(b) + [2] * len(c))\n",
    "    perm = rand.permutation(len(data_x))\n",
    "    return data_x[perm], data_y[perm]\n",
    "\n",
    "xs3_all, ys3_all = sample_data_3()\n",
    "\n",
    "for cls, clr in zip([0, 1, 2], 'rgb'):\n",
    "    plt.plot(xs3_all[ys3_all == cls, 0], xs3_all[ys3_all == cls, 1], clr + 'o')\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "n = xs3_all.shape[0]\n",
    "n_train = int(n * 0.8)\n",
    "xs3_train, ys3_train = xs3_all[:n_train], ys3_all[:n_train]\n",
    "xs3_val, ys3_val = xs3_all[n_train:], ys3_all[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "def collate_fn(data):\n",
    "    xs, ys = zip(*data)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "train_data3_loader = data.DataLoader(\n",
    "    dataset=list(zip(xs3_train, ys3_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_data3_loader = data.DataLoader(\n",
    "    dataset=list(zip(xs3_val, ys3_val)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latent(model, xs, ys):\n",
    "    xs = torch.tensor(xs, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        mu_z, var_z = model.encode(xs)\n",
    "        eps = eps = torch.randn_like(xs).to(device)\n",
    "        zs = mu_z + eps * torch.sqrt(var_z)\n",
    "        zs = zs.cpu().numpy()\n",
    "        \n",
    "    for cls, clr in zip([0, 1, 2], 'rgb'):\n",
    "        plt.plot(zs[ys == cls, 0], zs[ys == cls, 1], clr + 'o')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x, mu, var):\n",
    "    prob =  torch.exp(-0.5 * (x - mu) ** 2 / var) / torch.sqrt(np.pi * 2 * var)\n",
    "    return prob.prod(dim=-1)\n",
    "\n",
    "def calc_iwae(model, x, k=100):\n",
    "    with torch.no_grad():\n",
    "        mu_z, var_z = model.encode(x)\n",
    "        mu_z, var_z = mu_z[None, :, :], var_z[None, :, :]\n",
    "        \n",
    "        eps = torch.randn(k, *x.shape).to(device)\n",
    "        z = mu_z + eps * torch.sqrt(var_z)\n",
    "        \n",
    "        mu_x, var_x = model.decode(z.reshape([-1, 2]))\n",
    "        mu_x, var_x = mu_x.reshape(z.shape), var_x.reshape(z.shape)\n",
    "\n",
    "        p_x_z = prob(x, mu_x, var_x)\n",
    "        p_z = prob(z, torch.zeros_like(mu_z).to(device), torch.ones_like(var_z).to(device))\n",
    "        q_z_x = prob(z, mu_z, var_z)\n",
    "\n",
    "        w = ((p_x_z * p_z) / q_z_x).mean(dim=0)\n",
    "        iwae = torch.log(w).mean()\n",
    "        return iwae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0001\n",
    "w_l2 = 0\n",
    "\n",
    "model = Net1([2, 64, 64, 4])\n",
    "model.to(device)\n",
    "\n",
    "criterion_prob = ProbLoss()\n",
    "criterion_kl = KLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=w_l2)\n",
    "\n",
    "train(model, criterion_prob, criterion_kl,\n",
    "      optimizer, None, epochs,\n",
    "      train_data3_loader, val_data3_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_latent(model, xs3_all, ys3_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(xs3_val[:100], dtype=torch.float32, device=device)\n",
    "elbo = calc_iwae(model, xs, k=1)\n",
    "print('ELBO: {:.4f}'.format(elbo.item()))\n",
    "iwae = calc_iwae(model, xs, k=100)\n",
    "print('IWAE: {:.4f}'.format(iwae.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
